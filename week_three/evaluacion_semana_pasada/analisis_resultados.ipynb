{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Dataframe preguntas y respuestas que se esperan de los modelos\n",
    "df_preguntas_respuestas = pd.read_csv(\"preguntas_respuestas.csv\")\n",
    "df_preguntas_respuestas"
   ],
   "id": "4dcf3c68c8b99870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_resultados_experimentos = pd.read_csv(\"resultados_experimentos_modelos_prompts.csv\")\n",
    "df_resultados_experimentos"
   ],
   "id": "25f95ea1d6d1880e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Lista de etiquetas para las preguntas ===\n",
    "preguntas = [\n",
    "    \"Pregunta 1\", \"Pregunta 2\", \"Pregunta 3\", \"Pregunta 4\", \"Pregunta 5\",\n",
    "    \"Pregunta 6\", \"Pregunta 7\", \"Pregunta 8\", \"Pregunta 9\", \"Pregunta 10\",\n",
    "    \"Pregunta 11\", \"Pregunta 12\", \"Pregunta 13\", \"Pregunta 14\", \"Pregunta 15\"\n",
    "]\n",
    "\n",
    "# === Gráficas comparativas: todos los modelos para un mismo prompt ===\n",
    "prompts = df_resultados_experimentos[\"Prompt\"].unique()\n",
    "\n",
    "for prompt in prompts:\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    df_prompt = df_resultados_experimentos[df_resultados_experimentos[\"Prompt\"] == prompt]\n",
    "\n",
    "    for modelo in df_prompt[\"Modelo\"].unique():\n",
    "        df_modelo = df_prompt[df_prompt[\"Modelo\"] == modelo]\n",
    "        plt.plot(\n",
    "            range(1, len(df_modelo) + 1),\n",
    "            df_modelo[\"Tiempo_Respuesta\"],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            label=modelo\n",
    "        )\n",
    "\n",
    "    plt.xticks(range(1, len(df_prompt[\"Pregunta\"].unique()) + 1),\n",
    "               preguntas[:len(df_prompt[\"Pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.xlabel(\"Preguntas\")\n",
    "    plt.ylabel(\"Tiempo de respuesta (s)\")\n",
    "    plt.title(f\"Comparativa de modelos - {prompt}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "6cd034d3cfed6e8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_evaluacion = df_resultados_experimentos.copy()\n",
    "\n",
    "#Analisemos cuantas palabras tiene cada respuesta\n",
    "def normalizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Normaliza texto: minúsculas, remueve puntuación y espacios extra.\n",
    "    Adaptado para español (mantiene acentos básicos).\n",
    "    \"\"\"\n",
    "    if pd.isna(texto) or texto == \"\":\n",
    "        return \"\"\n",
    "    texto = str(texto).lower().strip()\n",
    "    texto = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', texto)  # Remueve puntuación, mantiene acentos y ñ\n",
    "    texto = re.sub(r'\\s+', ' ', texto)  # Normaliza espacios\n",
    "    return texto\n",
    "\n",
    "df_evaluacion[\"numero_palabras_respuesta\"] = df_evaluacion[\"Respuesta\"].apply(lambda x: len(normalizar_texto(x).split()))\n",
    "df_evaluacion\n"
   ],
   "id": "3e1cf1d0b2c5edc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Lista de etiquetas para las preguntas ===\n",
    "df_evaluacion_one=df_evaluacion.copy()\n",
    "preguntas = [\n",
    "    \"Pregunta 1\", \"Pregunta 2\", \"Pregunta 3\", \"Pregunta 4\", \"Pregunta 5\",\n",
    "    \"Pregunta 6\", \"Pregunta 7\", \"Pregunta 8\", \"Pregunta 9\", \"Pregunta 10\",\n",
    "    \"Pregunta 11\", \"Pregunta 12\", \"Pregunta 13\", \"Pregunta 14\", \"Pregunta 15\"\n",
    "]\n",
    "\n",
    "# === Gráficas comparativas: todos los modelos para un mismo prompt ===\n",
    "prompts = df_evaluacion_one[\"Prompt\"].unique()\n",
    "\n",
    "for prompt in prompts:\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    df_prompt = df_evaluacion_one[df_evaluacion_one[\"Prompt\"] == prompt]\n",
    "\n",
    "    for modelo in df_prompt[\"Modelo\"].unique():\n",
    "        df_modelo = df_prompt[df_prompt[\"Modelo\"] == modelo]\n",
    "        plt.plot(\n",
    "            range(1, len(df_modelo) + 1),\n",
    "            df_modelo[\"numero_palabras_respuesta\"],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            label=modelo\n",
    "        )\n",
    "\n",
    "    plt.xticks(range(1, len(df_prompt[\"Pregunta\"].unique()) + 1),\n",
    "               preguntas[:len(df_prompt[\"Pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.xlabel(\"Preguntas\")\n",
    "    plt.ylabel(\"Palabras en la respuesta\")\n",
    "    plt.title(f\"Comparativa de modelos - {prompt}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "a2ee13412d47555e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_resultados_deberta=pd.read_csv(\"resultados_mdeberta_two.csv\")\n",
    "df_resultados_deberta"
   ],
   "id": "90a916912d2d30f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_deberta_one=df_resultados_deberta.copy()\n",
    "preguntas = [\n",
    "    \"Pregunta 1\", \"Pregunta 2\", \"Pregunta 3\", \"Pregunta 4\", \"Pregunta 5\",\n",
    "    \"Pregunta 6\", \"Pregunta 7\", \"Pregunta 8\", \"Pregunta 9\", \"Pregunta 10\",\n",
    "    \"Pregunta 11\", \"Pregunta 12\", \"Pregunta 13\", \"Pregunta 14\", \"Pregunta 15\"\n",
    "]\n",
    "\n",
    "# === Gráficas comparativas: todos los modelos para un mismo prompt ===\n",
    "for prompt in prompts:\n",
    "    df_prompt = df_deberta_one[df_deberta_one[\"prompt\"] == prompt]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_prompt, x=\"pregunta\", y=\"es_correcta\", hue=\"modelo\", errorbar=None)\n",
    "\n",
    "    plt.xticks(range(0, len(df_prompt[\"pregunta\"].unique())),\n",
    "               preguntas[:len(df_prompt[\"pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.ylabel(\"Respuesta correcta (1 = Sí, 0 = No)\")\n",
    "    plt.title(f\"Comparativa binaria de modelos - {prompt}\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "1a1637d09d3b392a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_resultados_deberta=pd.read_csv(\"resultados_xlm_roberta.csv\")\n",
    "\n",
    "df_deberta_one=df_resultados_deberta.copy()\n",
    "preguntas = [\n",
    "    \"Pregunta 1\", \"Pregunta 2\", \"Pregunta 3\", \"Pregunta 4\", \"Pregunta 5\",\n",
    "    \"Pregunta 6\", \"Pregunta 7\", \"Pregunta 8\", \"Pregunta 9\", \"Pregunta 10\",\n",
    "    \"Pregunta 11\", \"Pregunta 12\", \"Pregunta 13\", \"Pregunta 14\", \"Pregunta 15\"\n",
    "]\n",
    "\n",
    "# === Gráficas comparativas: todos los modelos para un mismo prompt ===\n",
    "for prompt in prompts:\n",
    "    df_prompt = df_deberta_one[df_deberta_one[\"prompt\"] == prompt]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_prompt, x=\"pregunta\", y=\"es_correcta\", hue=\"modelo\", errorbar=None)\n",
    "\n",
    "    plt.xticks(range(0, len(df_prompt[\"pregunta\"].unique())),\n",
    "               preguntas[:len(df_prompt[\"pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.ylabel(\"Respuesta correcta (1 = Sí, 0 = No)\")\n",
    "    plt.title(f\"Comparativa binaria de modelos - {prompt}\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "24f6b1b497e9829a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Otros Analisis",
   "id": "96d1aa2a805839f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for prompt in prompts:\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    df_prompt = df_deberta_one[df_deberta_one[\"prompt\"] == prompt]\n",
    "\n",
    "    for modelo in df_prompt[\"modelo\"].unique():\n",
    "        df_modelo = df_prompt[df_prompt[\"modelo\"] == modelo]\n",
    "        plt.plot(\n",
    "            range(1, len(df_modelo) + 1),\n",
    "            df_modelo[\"confianza\"],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            label=modelo\n",
    "        )\n",
    "\n",
    "    plt.xticks(range(0, len(df_prompt[\"pregunta\"].unique())),\n",
    "               preguntas[:len(df_prompt[\"pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.ylabel(\"Confianza\")\n",
    "    plt.title(f\"Comparativa de modelos - {prompt}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "6f8ccf62b874aa04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for prompt in prompts:\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    df_prompt = df_deberta_one[df_deberta_one[\"prompt\"] == prompt]\n",
    "\n",
    "    for modelo in df_prompt[\"modelo\"].unique():\n",
    "        df_modelo = df_prompt[df_prompt[\"modelo\"] == modelo]\n",
    "        plt.plot(\n",
    "            range(1, len(df_modelo) + 1),\n",
    "            df_modelo[\"prob_entailment\"],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            label=modelo\n",
    "        )\n",
    "\n",
    "    plt.xticks(range(0, len(df_prompt[\"pregunta\"].unique())),\n",
    "               preguntas[:len(df_prompt[\"pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.ylabel(\"Probabilidad entailment\")\n",
    "    plt.title(f\"Comparativa de modelos - {prompt}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "e17444e36aadc875",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for prompt in prompts:\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    df_prompt = df_deberta_one[df_deberta_one[\"prompt\"] == prompt]\n",
    "\n",
    "    for modelo in df_prompt[\"modelo\"].unique():\n",
    "        df_modelo = df_prompt[df_prompt[\"modelo\"] == modelo]\n",
    "        plt.plot(\n",
    "            range(1, len(df_modelo) + 1),\n",
    "            df_modelo[\"prob_neutral\"],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            label=modelo\n",
    "        )\n",
    "\n",
    "    plt.xticks(range(0, len(df_prompt[\"pregunta\"].unique())),\n",
    "               preguntas[:len(df_prompt[\"pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.ylabel(\"Probabilidad neutral\")\n",
    "    plt.title(f\"Comparativa de modelos - {prompt}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4d22ed208e3d08fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for prompt in prompts:\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    df_prompt = df_deberta_one[df_deberta_one[\"prompt\"] == prompt]\n",
    "\n",
    "    for modelo in df_prompt[\"modelo\"].unique():\n",
    "        df_modelo = df_prompt[df_prompt[\"modelo\"] == modelo]\n",
    "        plt.plot(\n",
    "            range(1, len(df_modelo) + 1),\n",
    "            df_modelo[\"prob_contradiction\"],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            label=modelo\n",
    "        )\n",
    "\n",
    "    plt.xticks(range(1, len(df_prompt[\"pregunta\"].unique()) + 1),\n",
    "               preguntas[:len(df_prompt[\"pregunta\"].unique())],\n",
    "               rotation=90)\n",
    "    plt.ylabel(\"Probabilidad contradiction\")\n",
    "    plt.title(f\"Comparativa de modelos - {prompt}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "78f35e7cc7fef901",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Revision de Evaluacion de Pruebas\n",
    "\n",
    "Buscando en diferentes articulos y Benchmarks, de LLM's, me tope con evaluaciones comunes, por ejemplo\n",
    "las usadas en Human Last Exam, algunos de ellos son Framework's como HELM (Holistic Evaluation of Language Models) y otros metodos de evaluacion como MMLU (Massive Multitask Language Understanding) y BIG-Bench. En el siguiente Jupyter evaluare el desempeño de ellos con algunas preguntas que le he planteado a los modelos de lenguaje, con los cuales se hizo el estudio de ablacion de prompting."
   ],
   "id": "28b4e83bd67fd550"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Explicación del código\n",
    "\n",
    "El código implementa una **clase llamada `AutomaticEvaluator`**, cuyo propósito es **evaluar automáticamente la similitud entre dos textos**: uno considerado como **referencia** (por ejemplo, una respuesta ideal) y otro como **respuesta generada** (por ejemplo, por un modelo de lenguaje).\n",
    "\n",
    "Esta clase calcula varias **métricas de similitud textual** ampliamente utilizadas en el procesamiento de lenguaje natural (PLN).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Importación de librerías\n",
    "\n",
    "Se utilizan varias librerías de Python:\n",
    "\n",
    "* **TfidfVectorizer**: convierte los textos en vectores numéricos basados en la frecuencia de palabras (TF-IDF), lo que permite medir similitud entre textos.\n",
    "* **cosine_similarity**: calcula la similitud entre dos vectores (en este caso, los textos transformados con TF-IDF).\n",
    "* **rouge_scorer**: calcula las métricas **ROUGE**, muy usadas para evaluar la calidad de resúmenes automáticos y respuestas generadas.\n",
    "* **numpy**: permite realizar operaciones numéricas, como calcular promedios.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Definición de la clase *AutomaticEvaluator*\n",
    "\n",
    "La clase `AutomaticEvaluator` inicializa un evaluador automático que usa el paquete `rouge_scorer` para calcular tres métricas ROUGE:\n",
    "\n",
    "* **ROUGE-1**: mide la superposición de unigramas (palabras individuales).\n",
    "* **ROUGE-2**: mide la superposición de bigramas (pares consecutivos de palabras).\n",
    "* **ROUGE-L**: mide la longitud de la subsecuencia común más larga entre los textos.\n",
    "\n",
    "Además, se activa la opción *use_stemmer=True* para comparar las raíces de las palabras, de modo que “running” y “run” se consideren equivalentes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Cálculo de métricas de similitud\n",
    "\n",
    "El método principal, `calculate_similarity_metrics`, recibe dos cadenas de texto:\n",
    "\n",
    "* **reference**: el texto de referencia o respuesta esperada.\n",
    "* **response**: el texto generado que se desea evaluar.\n",
    "\n",
    "El método devuelve un conjunto de métricas que cuantifican la similitud entre ambos textos.\n",
    "\n",
    "#### a) Métricas ROUGE\n",
    "\n",
    "Se calculan las métricas **ROUGE-1**, **ROUGE-2** y **ROUGE-L**, obteniendo valores de precisión, recall y F-measure.\n",
    "En este caso, se utiliza únicamente el **F-measure**, que representa un equilibrio entre precisión y exhaustividad.\n",
    "\n",
    "#### b) Similitud del coseno (TF-IDF)\n",
    "\n",
    "Los textos se transforman en vectores mediante TF-IDF, y luego se calcula la **similitud del coseno**, que mide cuán alineados están esos vectores.\n",
    "\n",
    "El valor de la similitud del coseno varía entre:\n",
    "\n",
    "* **1**: textos idénticos o muy similares.\n",
    "* **0**: textos completamente diferentes.\n",
    "\n",
    "Esta métrica captura similitud semántica basada en la distribución de las palabras.\n",
    "\n",
    "#### c) Superposición de palabras\n",
    "\n",
    "Se calcula el porcentaje de **palabras compartidas** entre el texto de referencia y la respuesta, sin distinguir mayúsculas o minúsculas.\n",
    "Esta métrica simple ofrece una medida intuitiva del grado de coincidencia léxica.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Retorno de resultados\n",
    "\n",
    "El método devuelve un **diccionario de métricas**, que incluye:\n",
    "\n",
    "* ROUGE-1\n",
    "* ROUGE-2\n",
    "* ROUGE-L\n",
    "* Similitud del coseno (TF-IDF)\n",
    "* Superposición de palabras\n",
    "* Un **promedio global** entre las métricas ROUGE y la similitud del coseno\n",
    "\n",
    "Este valor promedio proporciona una evaluación general de la similitud textual entre la referencia y la respuesta generada.\n"
   ],
   "id": "29ce153a38efa2e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T21:47:17.211726474Z",
     "start_time": "2025-10-18T21:47:17.173846684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "\n",
    "class AutomaticEvaluator:\n",
    "    def __init__(self):\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    def calculate_similarity_metrics(self, reference: str, response: str) -> dict:\n",
    "        \"\"\"Calcula métricas automáticas de similitud\"\"\"\n",
    "\n",
    "        # ROUGE scores\n",
    "        rouge_scores = self.scorer.score(reference, response)\n",
    "\n",
    "        # Similitud coseno con TF-IDF\n",
    "        vectorizer = TfidfVectorizer().fit_transform([reference, response])\n",
    "        cosine_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])[0][0]\n",
    "\n",
    "        # Similitud por superposición de palabras\n",
    "        ref_words = set(reference.lower().split())\n",
    "        resp_words = set(response.lower().split())\n",
    "\n",
    "        if len(ref_words) > 0:\n",
    "            word_overlap = len(ref_words.intersection(resp_words)) / len(ref_words)\n",
    "        else:\n",
    "            word_overlap = 0\n",
    "\n",
    "        return {\n",
    "            'rouge1': round(float(rouge_scores['rouge1'].fmeasure),4),\n",
    "            'rouge2': round(float(rouge_scores['rouge2'].fmeasure),4),\n",
    "            'rougeL': round(float(rouge_scores['rougeL'].fmeasure),4),\n",
    "            'cosine_similarity': round(float(cosine_sim),4),\n",
    "            'word_overlap': round(float(word_overlap),4),\n",
    "            'score_promedio': round(float(np.mean([\n",
    "                rouge_scores['rouge1'].fmeasure,\n",
    "                rouge_scores['rouge2'].fmeasure,\n",
    "                rouge_scores['rougeL'].fmeasure,\n",
    "                cosine_sim\n",
    "            ])),4)\n",
    "        }\n",
    "if __name__ == \"__main__\":\n",
    "    # USO\n",
    "    auto_eval = AutomaticEvaluator()\n",
    "    metrics = auto_eval.calculate_similarity_metrics(\n",
    "        reference=\"\"\"Lo mas recomendable es coger la linea amarilla en sentido de ida\n",
    "     Las estaciones que cruzarás son: AA1SC → AB2SC → AC3SC → AD4RF → AE5VE → AF6SC → AG7BH\"\"\",\n",
    "        response=\"\"\"Puedes llegar desde la estación AA1SC a la estación AG7BH tomando la línea amarilla. Primero, dirígete hacia la estación AA1SC, que es la primera estación del sentido ida de la línea amarilla. Luego, sigue el recorrido de la línea amarilla en sentido ida: AA1SC, AB2SC, AC3SC, AD4RF, AE5VE, AF6SC y finalmente AG7BH.\"\"\"\n",
    "    )\n",
    "    print(metrics)\n"
   ],
   "id": "a860c2cbcb0111da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3529, 'rouge2': 0.1687, 'rougeL': 0.3059, 'cosine_similarity': 0.3908, 'word_overlap': 0.44, 'score_promedio': 0.3046}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "94b5447e33a5c198"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
